{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Data Collection\n",
        "\n",
        "### Data Intended for Machine Learning:\n",
        "\n",
        "Data intended for machine learning is a great place to start. This allows you to develop models without having to parse and clean raw data. If you're interested in building a full pipeline though, skip to *Data Not Intended For Machine Learning / Public Data*.\n",
        "\n",
        "Find a dataset, maybe from these popular databases:\n",
        "\n",
        "> Kaggle: https://www.kaggle.com/datasets\n",
        "\n",
        "> UCI Machine Learning Repository: https://archive.ics.uci.edu/datasets/\n",
        "\n",
        "> Hugging Face: https://huggingface.co/docs/hub/en/datasets\n",
        "\n",
        "> Microsoft Research Open Data: https://www.microsoft.com/en-us/research/tools/?\n",
        "\n",
        "> OpenML: https://www.openml.org/search?type=data&sort=runs&status=active\n",
        "\n",
        "> Papers With Code: https://paperswithcode.com/datasets\n",
        "\n",
        "> Standford Large Network Dataset Collection (for graph/network ML tasks): https://snap.stanford.edu/data/\n",
        "\n",
        "ML ready datasets are cleaned, contain clearly defined features, and are ready for experimentation. Start here if you prioritize the educational value of model tuning, architecture comparisons or performance benchmarking.\n",
        "\n",
        "### Data Not Intended for Machine Learning / Public Data:\n",
        "\n",
        "This is the side where cleaning, feature curation, and real-world problem framing comes in. This is the most important part of the pipeline, and the most excruciatingly difficult part of machine learning. Analyzing data, determining what features matter, how to make average data usable, is a skill that pays great dividends in data science.\n",
        "\n",
        "Find public data that's aligned with a problem or interest of yours. I'm into disaster recovery, the natural world, how people are affected by events... So I combined multiple public datasets from NOAA into one super-dataset.\n",
        "\n",
        "> Data.gov: https://data.gov/\n",
        "\n",
        "> World Health Organization: https://www.who.int/data\n",
        "\n",
        "> Open Data of New York City: https://opendata.cityofnewyork.us/\n",
        "\n",
        "> Any website with public data that interests you based on topic, proximity, potential worldy impact...\n",
        "\n",
        "**Your model is only as good as your data.**\n",
        "\n",
        "If you're just starting out, use clean datasets to learn the ropes. If you're an intermediate data scientist, dive into the mess. Parse the raw data, allow yourself to become confused. The more you work through ambiguity, the better you'll get at spotting patterns and interpreting relationships that can actually be modeled. And given enough time, you'll almost always find *something* meaningful in the noise. The next section actually goes into cleaning though, and what should actually be done to find patterns and make the data useful.\n",
        "\n",
        "Now, we can move onto Section 2: Cleaning & Feature Engineering"
      ],
      "metadata": {
        "id": "eueXsa1kSyt3"
      }
    }
  ]
}
