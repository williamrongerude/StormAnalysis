{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cleaning & Feature Engineering\n",
        "\n",
        "### Quality over Quantity\n",
        "Raw data is full of missing values, nan values, inconsistent formats, features you don't want and simply noisy columns. The goal is to condense this into a subset of features where each feature has predictive power.\n",
        "\n",
        "Start by opening your python IDE. Google Colab is recommended, though you can create a virtual enviornment in your IDE (VSCODE / Jupyter Notebook Locally) and run your code locally.\n",
        "\n",
        "Upload your dataset and put it into a pandas dataframe.\n",
        "\n",
        "Google Colab Upload:\n",
        "\n",
        "<img src=\"assets/2_process1.png\" width=\"200\">\n",
        "\n",
        "Locally Upload:\n",
        "\n",
        "<img src=\"assets/2_process2.png\" width=\"200\">\n",
        "\n",
        "Use df.head(10) to view the top 10 rows in your dataset.\n",
        "\n",
        "df.head() displays the entire dataset, which might take forever to run depending on your display settings.\n",
        "\n",
        "In colab, df.head() displays the top 5 and bottom 5 rows of your dataset. Use **pd.set_option('display.max_rows', None)**\n",
        "to display all rows.\n",
        "\n",
        "*What do you notice about the dataset below?*\n",
        "\n",
        "<img src=\"assets/2_dataset.png\" width=\"1200\">\n",
        "\n",
        "1. There are too many irrelevant columns.\n",
        "  > Columns like Episode ID, State Fips, CZ Timezone and WFO may not add value unless you're doing geographic modeling or event identification. If a column doesn't seem to contribute to a prediction / have any sort of numeric pattern with the target variable, drop it.\n",
        "2. Missing / NAN values\n",
        "  > Magnitude, every TOR column and damaged property have NAN values. We will replace nan values with better values, but for damaged property, we will simply remove rows where damage property is nan. This is because we need real target outputs to predict and we can't falsify true outputs.\n",
        "3. Incorrect Format\n",
        "  > Damaged Property is stored as a string representation of the float number it should be. 150.00K should be changed to 150,000. 10.00K should be 10,000.\n",
        "\n",
        "These are the most pressing issues right now. Lets address the irrelevant columns, and figure out how to remove NAN / missing values.\n",
        "\n",
        "### Remove Irrelevant Columns\n",
        "\n",
        "Use df.drop(columns=...) to drop columns. I like to store them in an list first:\n",
        "\n",
        "<img src=\"assets/2_dataset_command_to_drop.png\" width=\"400\">\n",
        "\n",
        "After dropping all my irrelevant columns:\n",
        "\n",
        "<img src=\"assets/2_dataset_dropped.png\" width=\"1200\">\n",
        "\n",
        "This is much better. The irrelevant features are gone, and I've kept features I know will have at least some predictive power over damaged property.\n",
        "\n",
        "My thought process is as follows: State definitely matters, considering certain states might have more destructive events. Event Type will definitely impact the amount of damage done. Event_Narrative, while not numeric, can be converted to numbers based off how strongly the event is described in words. I have faith in all these features, though I haven't necessarily tested them yet. This is a good starting point.\n",
        "\n",
        "### Impute Missing Values\n",
        "\n",
        "### Incorrect Format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Feature Engineering\n",
        "\n",
        "\n",
        "Now, we can move onto Section 3: Training, Testing and Modeling"
      ],
      "metadata": {
        "id": "4FdlEU-07L30"
      }
    }
  ]
}